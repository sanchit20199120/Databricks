-----------------------------------------Accessing DBFS-------------
%fs ls  ---> list all the files/ folder in the dbfs
%fs <'dbfs_folder_path'> --- > provide all the files in the particular folder within the DBFS


--------------Reading data from DBFS and creating databframe and processsing data---------------------------- 
Df1 = spark.read.csv('dbfs_file_path',
                     schema = 'order_id int, order_date string')

display(Df1)


Df2 = spark.read.csv('dbfs_file_path',
                     schema = 'order_id int, order_name string')

display(df2)

---Join, aggregating and filter----------

from pysaprk.sql.functions import sum, round

newresult = Df1. \
  filter('order_id in (1,2,3)). \
  join(Df2, Df1['order_id'] ==Df2['order_id']). \
  groupby('order_date'). \
  agg(round(sum('revenue_subtotal'),2).alias('total_revenue')). \
  orderby ('Order_date')

display(newresult)

-----------------Writing data to DBFS (persistent store)----------------------------

newresult. \
write. \
csv('dbfs_folder_path_plus_new_folder_name',header = True)

         
